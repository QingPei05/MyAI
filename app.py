import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile
from moviepy.editor import VideoFileClip

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(frame):
    """åˆ†æå•å¸§å›¾åƒçš„æƒ…ç»ªï¼ˆè¿”å›è¯¦ç»†ç»“æœå’Œæ ‡è®°åçš„å›¾åƒï¼‰"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    results = []
    marked_img = frame.copy()
    
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # é«˜çº§æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        emotion = "neutral"
        if len(smiles) > 0:
            smile_conf = len(smiles) / (w * h) * 1000  # å¾®ç¬‘å¯†åº¦
            if smile_conf > 0.5:
                emotion = "excited"
            else:
                emotion = "happy"
        elif len(eyes) > 0:
            eye_pos = eyes[0][1] / h  # çœ¼ç›ç›¸å¯¹ä½ç½®
            if eye_pos < 0.3:
                emotion = "sad"
            elif eye_pos > 0.7:
                emotion = "surprised"
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
        color = {
            "excited": (0, 255, 255),  # é»„è‰²
            "happy": (0, 255, 0),      # ç»¿è‰²
            "sad": (255, 0, 0),        # è“è‰²
            "surprised": (255, 0, 255),# ç²‰è‰²
            "neutral": (255, 255, 0)   # é’è‰²
        }.get(emotion, (255, 255, 255))
        
        cv2.rectangle(marked_img, (x, y), (x+w, y+h), color, 2)
        cv2.putText(marked_img, f"{emotion}", (x, y-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
        results.append({
            "position": [int(x), int(y), int(w), int(h)],
            "emotion": emotion,
            "confidence": {
                "smile_density": len(smiles) / (w * h) if len(smiles) > 0 else 0,
                "eye_position": eyes[0][1] / h if len(eyes) > 0 else 0.5
            }
        })
    
    return marked_img, results

def process_uploaded_file(uploaded_file):
    """è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æˆ–è§†é¢‘"""
    file_type = uploaded_file.type.split('/')[0]
    
    if file_type == "image":
        # å¤„ç†å›¾ç‰‡
        image = Image.open(uploaded_file)
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        marked_img, results = detect_emotion(img)
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2 = st.columns(2)
        with col1:
            st.image(image, caption="åŸå§‹å›¾ç‰‡", use_container_width=True)
        with col2:
            st.image(marked_img, channels="BGR", caption="åˆ†æç»“æœ", use_container_width=True)
        
        # æ˜¾ç¤ºè¯¦ç»†æƒ…ç»ªæ•°æ®
        st.subheader("æƒ…ç»ªåˆ†ææŠ¥å‘Š")
        for i, result in enumerate(results):
            st.markdown(f"""
            **äººè„¸ {i+1}**  
            - æƒ…ç»ª: `{result['emotion']}`  
            - ä½ç½®: `{result['position']}`  
            - å¾®ç¬‘å¼ºåº¦: `{result['confidence']['smile_density']:.2f}`  
            - çœ¼ç›ä½ç½®: `{result['confidence']['eye_position']:.2f}`
            """)
    
    elif file_type == "video":
        # å¤„ç†è§†é¢‘
        st.warning("è§†é¢‘å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_file.read())
            input_path = tmp.name
        
        # å¤„ç†è§†é¢‘å¹¶æ˜¾ç¤ºè¿›åº¦
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def process_frame(frame):
            marked_frame, _ = detect_emotion(frame)
            progress = min((frame_count / total_frames), 1.0)
            progress_bar.progress(progress)
            status_text.text(f"å·²å¤„ç† {frame_count}/{total_frames} å¸§...")
            return marked_frame
        
        clip = VideoFileClip(input_path)
        total_frames = int(clip.fps * clip.duration)
        frame_count = 0
        
        processed_clip = clip.fl_image(lambda f: process_frame(f))
        output_path = "output.mp4"
        processed_clip.write_videofile(output_path, codec="libx264", audio=False)
        
        # æ˜¾ç¤ºç»“æœ
        st.success("å¤„ç†å®Œæˆï¼")
        st.video(output_path)
        
        # æä¾›ä¸‹è½½
        with open(output_path, "rb") as f:
            st.download_button(
                label="ä¸‹è½½ç»“æœè§†é¢‘",
                data=f,
                file_name="emotion_analysis.mp4"
            )

def main():
    st.set_page_config(page_title="æ™ºèƒ½æƒ…ç»ªæ£€æµ‹", layout="wide")
    st.title("ğŸ­ AIæƒ…ç»ªåˆ†æç³»ç»Ÿ")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘ï¼ˆæ”¯æŒJPG/PNG/MP4ï¼‰", 
        type=["jpg", "png", "jpeg", "mp4"],
        accept_multiple_files=False
    )
    
    if uploaded_file:
        process_uploaded_file(uploaded_file)

if __name__ == "__main__":
    main()
