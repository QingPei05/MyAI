import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(frame):
    """å•å¸§æƒ…ç»ªæ£€æµ‹å‡½æ•°ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    results = []
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        eyes = eye_cascade.detectMultiScale(roi_gray)
        emotion = "neutral"
        if len(smiles) > 0:
            emotion = "happy"
        elif len(eyes) > 0 and eyes[0][1] < h/3:
            emotion = "sad"
        results.append({"box": [x,y,w,h], "emotion": emotion})
    return results

def process_video(video_path):
    """å¤„ç†è§†é¢‘æ–‡ä»¶çš„æ ¸å¿ƒå‡½æ•°"""
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    st.write(f"è§†é¢‘å‚æ•°: {int(fps)} FPS, æ€»å¸§æ•°: {int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}")
    
    # åˆ›å»ºè§†é¢‘è¾“å‡ºå™¨
    output_path = "output.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, 
                         (int(cap.get(3)), int(cap.get(4))))
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        # å¤„ç†æ¯ä¸€å¸§
        results = detect_emotion(frame)
        for result in results:
            x,y,w,h = result["box"]
            color = {"happy": (0,255,0), "sad": (0,0,255)}.get(result["emotion"], (255,255,0))
            cv2.rectangle(frame, (x,y), (x+w,y+h), color, 2)
            cv2.putText(frame, result["emotion"], (x,y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
        out.write(frame)
        frame_count += 1
        progress_bar.progress(frame_count / int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))
        status_text.text(f"å·²å¤„ç† {frame_count} å¸§...")
    
    cap.release()
    out.release()
    return output_path

def main():
    st.set_page_config(page_title="è§†é¢‘æƒ…ç»ªæ£€æµ‹", layout="wide")
    st.title("ğŸ¥ è§†é¢‘æƒ…ç»ªåˆ†æç³»ç»Ÿ")
    
    # æ¨¡å¼é€‰æ‹©
    analysis_mode = st.radio(
        "é€‰æ‹©è¾“å…¥ç±»å‹",
        ["å›¾ç‰‡æ£€æµ‹", "è§†é¢‘æ£€æµ‹"],
        horizontal=True
    )
    
    if analysis_mode == "å›¾ç‰‡æ£€æµ‹":
        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=["jpg", "png"])
        if uploaded_file:
            image = Image.open(uploaded_file)
            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            results = detect_emotion(img)
            detected_img = img.copy()
            for result in results:
                x,y,w,h = result["box"]
                color = {"happy": (0,255,0), "sad": (0,0,255)}.get(result["emotion"], (255,255,0))
                cv2.rectangle(detected_img, (x,y), (x+w,y+h), color, 2)
            st.image(detected_img, channels="BGR", caption="æ£€æµ‹ç»“æœ")
    
    else:  # è§†é¢‘æ£€æµ‹æ¨¡å¼
        uploaded_video = st.file_uploader("ä¸Šä¼ è§†é¢‘", type=["mp4", "avi"])
        if uploaded_video:
            # ä¿å­˜ä¸´æ—¶è§†é¢‘æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
                tmp.write(uploaded_video.read())
                video_path = tmp.name
            
            st.video(uploaded_video)
            if st.button("å¼€å§‹åˆ†æè§†é¢‘"):
                output_path = process_video(video_path)
                st.success("åˆ†æå®Œæˆï¼")
                st.video(output_path, format="video/mp4")

if __name__ == "__main__":
    main()
