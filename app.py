import cv2
import numpy as np
import streamlit as st
from PIL import Image

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„å‚æ•°ï¼‰
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(img):
    """é«˜ç²¾åº¦æƒ…ç»ªæ£€æµ‹ï¼ˆå¿«ä¹ã€å¹³é™ã€æ‚²ä¼¤ã€æ„¤æ€’ï¼‰"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # é«˜ç²¾åº¦äººè„¸æ£€æµ‹å‚æ•°
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.05,
        minNeighbors=10,
        minSize=(150, 150),
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    
    emotions = []
    valid_faces = []
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        valid_faces.append((x,y,w,h))
        
        # é«˜ç²¾åº¦ç‰¹å¾æ£€æµ‹
        smiles = smile_cascade.detectMultiScale(
            roi_gray,
            scaleFactor=1.7,
            minNeighbors=25,
            minSize=(40, 40)
        )
        
        eyes = eye_cascade.detectMultiScale(
            roi_gray,
            scaleFactor=1.1,
            minNeighbors=8,
            minSize=(45, 45))
        )
        
        # å¤šç»´åº¦æƒ…ç»ªåˆ¤æ–­
        emotion = "å¹³é™"
        eye_features = {"count": 0, "avg_height": 0, "avg_size": 0}
        
        if len(eyes) >= 2:
            eye_features = {
                "count": len(eyes),
                "avg_height": np.mean([y + ey + eh/2 for (ex,ey,ew,eh) in eyes[:2]]),
                "avg_size": np.mean([eh for (ex,ey,ew,eh) in eyes[:2]])
            }
            
            # æ„¤æ€’åˆ¤æ–­ï¼ˆçœ¼ç›å¤§å°å’Œä½ç½®ï¼‰
            if eye_features["avg_size"] > h/4.5 and eye_features["avg_height"] < h/2.2:
                emotion = "æ„¤æ€’"
            # æ‚²ä¼¤åˆ¤æ–­ï¼ˆçœ¼ç›ä½ç½®å’Œå˜´å·´ï¼‰
            elif eye_features["avg_height"] > h/2.2:
                emotion = "æ‚²ä¼¤"
        
        # å¿«ä¹åˆ¤æ–­ï¼ˆç¬‘å®¹è´¨é‡ï¼‰
        if len(smiles) > 0:
            main_smile = max(smiles, key=lambda s: s[2]*s[3])  # é€‰æ‹©æœ€å¤§é¢ç§¯çš„ç¬‘å®¹
            smile_ratio = main_smile[2] / w
            if smile_ratio > 0.35 and main_smile[3] > h/6:  # ç¬‘å®¹å®½åº¦å’Œé«˜åº¦é˜ˆå€¼
                emotion = "å¿«ä¹"
                # å¿«ä¹ç¨‹åº¦åˆ†çº§
                if smile_ratio > 0.45 and eye_features.get("avg_size", 0) > h/5:
                    emotion = "å¿«ä¹"  # å¯æ‰©å±•ä¸º"éå¸¸å¿«ä¹"
        
        emotions.append(emotion)
    
    return emotions, valid_faces

def draw_detections(img, emotions, faces):
    """é«˜å¯è§æ€§æ ‡æ³¨ç»˜åˆ¶"""
    output_img = img.copy()
    for i, ((x,y,w,h), emotion) in enumerate(zip(faces, emotions)):
        # é¢œè‰²æ˜ å°„
        color_map = {
            "å¿«ä¹": (0, 180, 0),
            "å¹³é™": (210, 210, 0),
            "æ‚²ä¼¤": (0, 0, 180),
            "æ„¤æ€’": (0, 100, 255)
        }
        color = color_map.get(emotion, (150,150,150))
        
        # å¸¦èƒŒæ™¯çš„æ–‡æœ¬æ ‡ç­¾
        text = f"{i+1}:{emotion}"
        (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
        cv2.rectangle(output_img, (x, y-40), (x+text_w+10, y-10), color, -1)
        cv2.putText(output_img, text, (x+5, y-20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
        
        # äººè„¸æ¡†
        cv2.rectangle(output_img, (x,y), (x+w,y+h), color, 3)
        
        # ç‰¹å¾ç‚¹æ ‡è®°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        # if emotion == "æ„¤æ€’":
        #     cv2.circle(output_img, (x+w//2, y+h//2), 5, (0,0,255), -1)
    
    return output_img

def main():
    st.set_page_config(page_title="é«˜ç²¾åº¦æƒ…ç»ªæ£€æµ‹", layout="wide")
    st.title("ğŸ˜Š é«˜ç²¾åº¦æƒ…ç»ªæ£€æµ‹")
    
    uploaded_file = st.file_uploader("ä¸Šä¼ æ¸…æ™°æ­£è„¸ç…§ç‰‡ï¼ˆJPG/PNGï¼‰", type=["jpg", "png"])
    
    if uploaded_file:
        try:
            # å›¾åƒé¢„å¤„ç†
            image = Image.open(uploaded_file)
            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # æ£€æµ‹æƒ…ç»ª
            emotions, faces = detect_emotion(img)
            detected_img = draw_detections(img.copy(), emotions, faces)
            
            # ä¿æŒæ‚¨å–œæ¬¢çš„å¸ƒå±€
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("æ£€æµ‹ç»“æœ")
                if emotions:
                    emotion_count = {
                        "å¿«ä¹": emotions.count("å¿«ä¹"),
                        "å¹³é™": emotions.count("å¹³é™"),
                        "æ‚²ä¼¤": emotions.count("æ‚²ä¼¤"),
                        "æ„¤æ€’": emotions.count("æ„¤æ€’")
                    }
                    
                    result = []
                    for emo, cnt in emotion_count.items():
                        if cnt > 0:
                            result.append(f"{cnt}äºº{emo}")
                    st.success("ï¼Œ".join(result))
                    
                    st.markdown("---")
                    st.markdown("**ä¼˜åŒ–è¯´æ˜**ï¼š")
                    st.write("""
                    - ä½¿ç”¨150x150åƒç´ æœ€å°äººè„¸æ£€æµ‹
                    - å¤šç»´åº¦ç‰¹å¾äº¤å‰éªŒè¯
                    - ç¬‘å®¹è´¨é‡åˆ†çº§åˆ¤æ–­
                    """)
                else:
                    st.warning("æœªæ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸ï¼Œè¯·å°è¯•ï¼š\n1. æ­£å¯¹æ‘„åƒå¤´\n2. ä¿æŒè‰¯å¥½å…‰ç…§\n3. é¿å…é®æŒ¡")
            
            with col2:
                tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
                with tab1:
                    st.image(image, use_container_width=True, caption="åŸå§‹å›¾ç‰‡")
                with tab2:
                    st.image(detected_img, channels="BGR", use_container_width=True,
                           caption=f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸ | æƒ…ç»ªæ ‡è®°: åºå·:æƒ…ç»ª")
                
        except Exception as e:
            st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            st.info("å»ºè®®ä¸Šä¼ æ›´æ¸…æ™°çš„ç…§ç‰‡ï¼Œé¿å…ä¾§è„¸æˆ–æ¨¡ç³Š")

if __name__ == "__main__":
    main()
