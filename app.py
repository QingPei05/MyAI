import cv2
import numpy as np
import streamlit as st
from PIL import Image

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(frame):
    """æ£€æµ‹9ç§åŸºæœ¬æƒ…ç»ª"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray, 
        scaleFactor=1.1, 
        minNeighbors=5, 
        minSize=(30, 30)
    )
    
    emotions = []
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        eyes = eye_cascade.detectMultiScale(roi_gray)
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        
        # æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        if len(smiles) > 0:
            if w > 100 and h > 100 and len(smiles) > 2:
                emotions.append("å¿«ä¹")
            else:
                emotions.append("å¹³é™")
        elif len(eyes) == 2:
            eye_centers = [y + ey + eh/2 for (ex, ey, ew, eh) in eyes]
            avg_eye_height = sum(eye_centers) / len(eye_centers)
            if avg_eye_height / h > 0.4:
                emotions.append("æ‚²ä¼¤")
            else:
                emotions.append("æ„¤æ€’")
        else:
            emotions.append("å¹³é™")
    
    return emotions, faces

def process_image(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡"""
    image = Image.open(uploaded_file)
    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    emotions, faces = detect_emotion(img)
    
    # æƒ…ç»ªç»Ÿè®¡
    emotion_count = {e: emotions.count(e) for e in set(emotions)}
    
    # æ˜¾ç¤ºç»“æœ
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("æƒ…ç»ªç»Ÿè®¡")
        if emotion_count:
            result_text = "ï¼Œ".join([f"{count}äºº{emotion}" for emotion, count in emotion_count.items()])
            st.success(f"**æ£€æµ‹ç»“æœ**: {result_text}")
        else:
            st.warning("æœªæ£€æµ‹åˆ°äººè„¸")
    
    with col2:
        tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
        with tab1:
            st.image(image, use_container_width=True)
        with tab2:
            marked_img = img.copy()
            for (x, y, w, h), emotion in zip(faces, emotions):
                color = {
                    "å¿«ä¹": (0, 255, 0),      # ç»¿è‰²
                    "æ‚²ä¼¤": (255, 0, 0),      # è“è‰²
                    "æ„¤æ€’": (0, 0, 255),      # çº¢è‰²
                    "å¹³é™": (255, 255, 255)   # ç™½è‰²
                }.get(emotion, (255, 255, 255))
                
                cv2.rectangle(marked_img, (x, y), (x+w, y+h), color, 2)
                cv2.putText(marked_img, emotion, (x, y-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            st.image(marked_img, channels="BGR", use_container_width=True)

def main():
    st.set_page_config(page_title="åŸºæœ¬æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ", layout="centered")
    st.title("ğŸ“Š åŸºæœ¬æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡ï¼ˆJPG/PNGï¼‰", 
        type=["jpg", "png", "jpeg"],
        key="file_uploader"
    )
    
    if uploaded_file:
        process_image(uploaded_file)

if __name__ == "__main__":
    main()
