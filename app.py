import cv2
import numpy as np
import streamlit as st
from PIL import Image

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(img):
    """ä½¿ç”¨OpenCVæ£€æµ‹æƒ…ç»ªï¼ˆå¿«ä¹ã€å¹³é™ã€æ‚²ä¼¤ã€æ„¤æ€’ï¼‰"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    emotions = []
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹å¾®ç¬‘
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        # æ£€æµ‹çœ¼ç›
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        emotion = "å¹³é™"  # é»˜è®¤
        
        # æ„¤æ€’åˆ¤æ–­ï¼ˆæ–°å¢ï¼‰
        if len(eyes) >= 2:
            eye_centers = [y + ey + eh/2 for (ex,ey,ew,eh) in eyes[:2]]
            avg_eye_height = np.mean(eye_centers)
            eye_sizes = [eh for (ex,ey,ew,eh) in eyes[:2]]
            avg_eye_size = np.mean(eye_sizes)
            
            if avg_eye_size > h/5 and avg_eye_height < h/2.5:
                emotion = "æ„¤æ€’"
            elif avg_eye_height < h/3:
                emotion = "æ‚²ä¼¤"
        
        # å¿«ä¹åˆ¤æ–­ï¼ˆä¼˜å…ˆåˆ¤æ–­ï¼‰
        if len(smiles) > 0:
            emotion = "å¿«ä¹"
        
        emotions.append(emotion)
    
    return emotions, faces

def draw_detections(img, emotions, faces):
    """ä¼˜åŒ–åçš„æ ‡ç­¾ç»˜åˆ¶å‡½æ•°"""
    output_img = img.copy()
    for i, ((x,y,w,h), emotion) in enumerate(zip(faces, emotions)):
        # æ›´é†’ç›®çš„é¢œè‰²æ˜ å°„
        color_map = {
            "å¿«ä¹": (0, 200, 0),     # æ›´äº®çš„ç»¿è‰²
            "å¹³é™": (255, 255, 100),  # æ›´äº®çš„é»„è‰²
            "æ‚²ä¼¤": (200, 50, 50),    # æ›´æŸ”å’Œçš„çº¢è‰²
            "æ„¤æ€’": (255, 150, 50)    # æ›´äº®çš„æ©™è‰²
        }
        color = color_map.get(emotion, (200, 200, 200))
        
        # ä¼˜åŒ–æ ‡ç­¾æ ·å¼
        text = f"{emotion}"  # ç§»é™¤äº†åºå·ï¼Œåªæ˜¾ç¤ºæƒ…ç»ª
        font_scale = 0.9 if w > 100 else 0.7  # æ ¹æ®äººè„¸å¤§å°è°ƒæ•´å­—ä½“
        
        # è®¡ç®—æ–‡æœ¬å¤§å°å’Œä½ç½®
        (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)
        
        # ç»˜åˆ¶èƒŒæ™¯æ¡†ï¼ˆåœ†è§’çŸ©å½¢æ•ˆæœï¼‰
        cv2.rectangle(output_img, 
                     (x, y - text_h - 20), 
                     (x + text_w + 20, y - 10), 
                     color, -1)
        
        # ç»˜åˆ¶æ–‡å­—ï¼ˆå±…ä¸­æ˜¾ç¤ºï¼‰
        cv2.putText(output_img, text, 
                   (x + 10, y - 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, 
                   (255, 255, 255), 2, cv2.LINE_AA)
        
        # ç»˜åˆ¶äººè„¸æ¡†ï¼ˆæ›´ç²—çš„çº¿æ¡ï¼‰
        cv2.rectangle(output_img, (x,y), (x+w,y+h), color, 3)
    
    return output_img

def main():
    st.set_page_config(page_title="æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ", layout="wide")
    st.title("ğŸ˜Š æƒ…ç»ªæ£€æµ‹")
    
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ï¼ˆJPG/PNGï¼‰", type=["jpg", "png"])
    
    if uploaded_file:
        try:
            # è½¬æ¢å›¾ç‰‡æ ¼å¼
            image = Image.open(uploaded_file)
            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # æ£€æµ‹æƒ…ç»ª
            emotions, faces = detect_emotion(img)
            detected_img = draw_detections(img.copy(), emotions, faces)
            
            # ä½¿ç”¨ä¸¤åˆ—å¸ƒå±€ï¼ˆå·¦ä¾§ç»“æœï¼Œå³ä¾§å›¾ç‰‡ï¼‰
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.subheader("æ£€æµ‹ç»“æœ")
                if emotions:
                    # ä¸­æ–‡å­—å…¸æ˜ å°„
                    emotion_mapping = {
                        "å¿«ä¹": "å¼€å¿ƒ",
                        "å¹³é™": "å¹³é™",
                        "æ‚²ä¼¤": "ä¼¤å¿ƒ",
                        "æ„¤æ€’": "æ„¤æ€’"
                    }
                    
                    emotion_count = {
                        "å¼€å¿ƒ": emotions.count("å¿«ä¹"),
                        "å¹³é™": emotions.count("å¹³é™"),
                        "ä¼¤å¿ƒ": emotions.count("æ‚²ä¼¤"),
                        "æ„¤æ€’": emotions.count("æ„¤æ€’")
                    }
                    
                    result = []
                    for emo, cnt in emotion_count.items():
                        if cnt > 0:
                            result.append(f"{cnt}äºº{emo}")
                    st.success("ï¼Œ".join(result))
                    
                    st.markdown("---")
                    st.markdown("**æ£€æµ‹åŸç†**ï¼š")
                    st.write("""
                    - ğŸ˜Š å¼€å¿ƒ: æ£€æµ‹åˆ°æ˜æ˜¾ç¬‘å®¹
                    - ğŸ˜  æ„¤æ€’: çœ¼ç›çå¤§ä¸”ä½ç½®åé«˜
                    - ğŸ˜ å¹³é™: é»˜è®¤ä¸­æ€§è¡¨æƒ…
                    - ğŸ˜¢ ä¼¤å¿ƒ: çœ¼ç›ä½ç½®åé«˜
                    """)
                else:
                    st.warning("æœªæ£€æµ‹åˆ°äººè„¸")
            
            with col2:
                # ä½¿ç”¨é€‰é¡¹å¡æ˜¾ç¤ºå›¾ç‰‡
                tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
                with tab1:
                    st.image(image, use_container_width=True)
                with tab2:
                    st.image(detected_img, channels="BGR", use_container_width=True,
                           caption=f"æ£€æµ‹åˆ° {len(faces)} ä¸ªäººè„¸")  # ä¿®æ”¹ä¸ºåªæ˜¾ç¤ºäººè„¸æ•°é‡
                
        except Exception as e:
            st.error(f"å¤„ç†é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()
