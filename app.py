import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile
from moviepy.editor import VideoFileClip

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

def detect_emotion(frame):
    """åˆ†æå•å¸§å›¾åƒçš„æƒ…ç»ªï¼ˆä»…è¿”å›æƒ…ç»ªæ ‡ç­¾ï¼‰"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    emotions = []
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        if len(smiles) > 3:  # å¤šä¸ªå¾®ç¬‘åŒºåŸŸ
            emotions.append("å…´å¥‹")
        elif len(smiles) > 0:
            emotions.append("å¼€å¿ƒ")
        elif len(eyes) > 0 and eyes[0][1] / h < 0.3:  # çœ¼ç›ä½ç½®åé«˜
            emotions.append("éš¾å—")
        else:
            emotions.append("ä¸­æ€§")
    
    return emotions

def process_uploaded_file(uploaded_file):
    """è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æˆ–è§†é¢‘"""
    file_type = uploaded_file.type.split('/')[0]
    
    if file_type == "image":
        # å¤„ç†å›¾ç‰‡
        image = Image.open(uploaded_file)
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        emotions = detect_emotion(img)
        
        # æ˜¾ç¤ºç»“æœ
        col1, col2 = st.columns(2)
        with col1:
            st.image(image, caption="åŸå§‹å›¾ç‰‡", use_container_width=True)
        
        # ç»Ÿè®¡æƒ…ç»ª
        emotion_count = {}
        for e in emotions:
            emotion_count[e] = emotion_count.get(e, 0) + 1
        
        # ç®€åŒ–è¾“å‡ºæ ¼å¼
        with col2:
            st.image(img, channels="BGR", caption="åˆ†æç»“æœ", use_container_width=True)
            st.subheader("æƒ…ç»ªç»Ÿè®¡")
            if emotion_count:
                result_text = "ï¼Œ".join([f"{count}äºº{emotion}" for emotion, count in emotion_count.items()])
                st.success(f"**æ£€æµ‹ç»“æœ**: {result_text}")
            else:
                st.warning("æœªæ£€æµ‹åˆ°äººè„¸")

    elif file_type == "video":
        # å¤„ç†è§†é¢‘
        st.warning("è§†é¢‘å¤„ç†ä¸­...ï¼ˆè‡ªåŠ¨æˆªå–å‰10ç§’ï¼‰")
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_file.read())
            input_path = tmp.name
        
        # åªå¤„ç†å‰10ç§’
        clip = VideoFileClip(input_path).subclip(0, min(10, VideoFileClip(input_path).duration))
        total_emotions = []
        
        # é™å¸§å¤„ç†ï¼ˆ5FPSï¼‰
        for frame in clip.iter_frames(fps=5):
            frame = cv2.resize(frame, (640, 360))  # é™ä½åˆ†è¾¨ç‡åŠ é€Ÿå¤„ç†
            total_emotions.extend(detect_emotion(frame))
        
        # ç»Ÿè®¡å…¨å±€æƒ…ç»ª
        emotion_count = {}
        for e in total_emotions:
            emotion_count[e] = emotion_count.get(e, 0) + 1
        
        # æ˜¾ç¤ºç»“æœ
        st.success("åˆ†æå®Œæˆï¼")
        if emotion_count:
            result_text = "ï¼Œ".join([f"{count}äºº{emotion}" for emotion, count in emotion_count.items()])
            st.markdown(f"**æœ€ç»ˆç»Ÿè®¡**: {result_text}")
            
            # æ˜¾ç¤ºç¤ºä¾‹å¸§
            st.video(input_path)
        else:
            st.warning("è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°äººè„¸")

def main():
    st.set_page_config(page_title="æç®€æƒ…ç»ªæ£€æµ‹", layout="centered")
    st.title("ğŸ˜Š æƒ…ç»ªå¿«æ£€ç³»ç»Ÿ")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘ï¼ˆJPG/PNG/MP4ï¼‰", 
        type=["jpg", "png", "jpeg", "mp4"],
        key="file_uploader"
    )
    
    if uploaded_file:
        process_uploaded_file(uploaded_file)

if __name__ == "__main__":
    main()
