import cv2
import numpy as np
import streamlit as st
from PIL import Image

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')

def detect_emotion(img):
    """ä½¿ç”¨OpenCVæ£€æµ‹æƒ…ç»ªï¼ˆhappy/neutral/sadï¼‰"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    results = []
    for (x,y,w,h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = img[y:y+h, x:x+w]
        
        # æ£€æµ‹å¾®ç¬‘
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        # æ£€æµ‹çœ¼ç›
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        emotion = "neutral"  # é»˜è®¤ä¸­æ€§
        if len(smiles) > 0:
            emotion = "happy"
        elif len(eyes) > 0:
            if eyes[0][1] < h/3:  # çœ¼ç›ä½ç½®åé«˜
                emotion = "sad"
        
        results.append({
            "box": [x,y,w,h],
            "emotion": emotion,
            "landmarks": {
                "eyes": [(x+ex, y+ey, ew, eh) for (ex,ey,ew,eh) in eyes],
                "smiles": [(x+sx, y+sy, sw, sh) for (sx,sy,sw,sh) in smiles]
            }
        })
    
    return results

def draw_detections(img, results):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
    for result in results:
        x,y,w,h = result["box"]
        
        # ç»˜åˆ¶äººè„¸æ¡†ï¼ˆé¢œè‰²æ ¹æ®æƒ…ç»ªå˜åŒ–ï¼‰
        color = {
            "happy": (0, 255, 0),    # ç»¿è‰²
            "neutral": (255, 255, 0), # é»„è‰²
            "sad": (0, 0, 255)       # çº¢è‰²
        }.get(result["emotion"], (255,255,255))
        
        cv2.rectangle(img, (x,y), (x+w,y+h), color, 2)
        
        # æ ‡è®°æƒ…ç»ªæ–‡æœ¬
        cv2.putText(img, result["emotion"], (x, y-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)
        
        # ç»˜åˆ¶çœ¼ç›å’Œå¾®ç¬‘åŒºåŸŸ
        for (ex,ey,ew,eh) in result["landmarks"]["eyes"]:
            cv2.rectangle(img, (ex,ey), (ex+ew,ey+eh), (255,0,0), 1)
        for (sx,sy,sw,sh) in result["landmarks"]["smiles"]:
            cv2.rectangle(img, (sx,sy), (sx+sw,sy+sh), (0,255,255), 1)
    
    return img

def main():
    st.set_page_config(page_title="OpenCVæƒ…ç»ªæ£€æµ‹", layout="wide")
    st.title("ğŸ˜Š å®æ—¶æƒ…ç»ªåˆ†æ")
    
    # æ¨¡å¼é€‰æ‹©
    analysis_mode = st.radio(
        "é€‰æ‹©è¾“å…¥æ¨¡å¼",
        ["ä¸Šä¼ å›¾ç‰‡", "å®æ—¶æ‘„åƒå¤´"],
        horizontal=True
    )
    
    if analysis_mode == "ä¸Šä¼ å›¾ç‰‡":
        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡ï¼ˆJPG/PNGï¼‰", type=["jpg", "png"])
        if uploaded_file:
            try:
                # è½¬æ¢å›¾ç‰‡æ ¼å¼
                image = Image.open(uploaded_file)
                img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                
                # æ£€æµ‹æƒ…ç»ª
                results = detect_emotion(img)
                detected_img = draw_detections(img.copy(), results)
                
                # æ˜¾ç¤ºç»“æœ
                col1, col2 = st.columns(2)
                with col1:
                    st.image(image, caption="åŸå§‹å›¾ç‰‡", use_column_width=True)
                with col2:
                    st.image(detected_img, channels="BGR", caption="åˆ†æç»“æœ", use_column_width=True)
                
                # æ–‡å­—ç»“æœ
                for i, result in enumerate(results):
                    st.markdown(f"**äººè„¸ {i+1}**:")
                    st.write(f"- æƒ…ç»ª: `{result['emotion']}`")
                    st.write(f"- ä½ç½®: `{result['box']}`")
                    
            except Exception as e:
                st.error(f"å¤„ç†é”™è¯¯: {str(e)}")
    
    else:  # å®æ—¶æ‘„åƒå¤´æ¨¡å¼
        st.warning("æ³¨æ„ï¼šæ‘„åƒå¤´åŠŸèƒ½éœ€è¦æœ¬åœ°è¿è¡Œæˆ–å¯ç”¨æµè§ˆå™¨æƒé™")
        run_camera = st.checkbox("å¯åŠ¨æ‘„åƒå¤´")
        frame_placeholder = st.empty()
        
        if run_camera:
            cap = cv2.VideoCapture(0)
            stop_button = st.button("åœæ­¢")
            
            while cap.isOpened() and not stop_button:
                ret, frame = cap.read()
                if not ret:
                    st.error("æ— æ³•è·å–æ‘„åƒå¤´ç”»é¢")
                    break
                
                # å®æ—¶åˆ†æ
                results = detect_emotion(frame)
                detected_frame = draw_detections(frame, results)
                
                # æ˜¾ç¤ºå®æ—¶ç”»é¢
                frame_placeholder.image(detected_frame, channels="BGR")
            
            cap.release()
            cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
