import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile
from moviepy.editor import VideoFileClip

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

def detect_emotion(frame):
    """ä½¿ç”¨ç²¾ç¡®è§„åˆ™åˆ†æå•å¸§å›¾åƒçš„æƒ…ç»ª"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray = cv2.equalizeHist(gray)
    
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    
    emotions = []
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        smiles = smile_cascade.detectMultiScale(
            roi_gray, 
            scaleFactor=1.8, 
            minNeighbors=20,
            minSize=(25, 25))
        
        eyes = eye_cascade.detectMultiScale(
            roi_gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(20, 20))
        
        eye_count = len(eyes)
        smile_count = len(smiles)
        eye_positions = [eye[1] for eye in eyes] if eye_count > 0 else []
        avg_eye_position = sum(eye_positions)/len(eye_positions) if eye_positions else 0
        
        # ç²¾ç¡®çš„å¤šå±‚æ¬¡æƒ…ç»ªåˆ¤æ–­
        if smile_count > 3:
            if eye_count > 1 and avg_eye_position < h * 0.3:
                emotions.append("çˆ±")
            else:
                emotions.append("å¿«ä¹")
        elif smile_count > 1:
            if eye_count > 1 and any(e[3] > h * 0.25 for e in eyes):  # å¤§çœ¼ç›
                emotions.append("å…´å¥‹")
            else:
                emotions.append("æ»¡è¶³")
        elif smile_count > 0:
            emotions.append("å¹³é™")
        elif eye_count > 1:
            if avg_eye_position > h * 0.6:
                if all(e[3] < h * 0.2 for e in eyes):  # å°çœ¼ç›
                    emotions.append("æ‚²ä¼¤")
                else:
                    emotions.append("ç¾æ„§")
            elif avg_eye_position < h * 0.3:
                if w > h * 0.85:  # å®½è„¸
                    emotions.append("æ„¤æ€’")
                else:
                    emotions.append("å«‰å¦’")
            else:
                if any(e[3] > h * 0.25 for e in eyes):  # å¤§çœ¼ç›
                    emotions.append("ææƒ§")
                else:
                    emotions.append("ç„¦è™‘")
        elif eye_count == 1:
            emotions.append("å°´å°¬")
        else:
            # æ— ç‰¹å¾æ—¶çš„ä¿å®ˆåˆ¤æ–­
            if w > h * 0.85:  # å®½è„¸
                emotions.append("éª„å‚²")
            elif h > w * 1.4:  # é•¿è„¸
                emotions.append("å†…ç–š")
            else:
                emotions.append("ä¸­æ€§")
    
    return emotions

def process_frame(frame):
    """å¤„ç†å•å¸§å›¾åƒå¹¶è¿”å›æ ‡è®°åçš„å›¾åƒ"""
    emotions = detect_emotion(frame)
    marked_img = frame.copy()
    
    # å®Œæ•´çš„æƒ…ç»ªé¢œè‰²æ˜ å°„
    emotion_colors = {
        "å¿«ä¹": (0, 255, 0),      # ç»¿è‰²
        "æ‚²ä¼¤": (0, 0, 255),      # çº¢è‰²
        "æ„¤æ€’": (0, 0, 139),     # æ·±çº¢è‰²
        "ææƒ§": (255, 0, 0),     # è“è‰²
        "åŒæ¶": (139, 0, 139),   # ç´«è‰²
        "æƒŠè®¶": (255, 255, 0),   # é’è‰²
        "çˆ±": (255, 105, 180),  # ç²‰è‰²
        "å†…ç–š": (165, 42, 42),   # æ£•è‰²
        "ç¾æ„§": (218, 165, 32),  # é‡‘è‰²
        "å°´å°¬": (255, 192, 203), # ç²‰çº¢
        "éª„å‚²": (255, 215, 0),   # é‡‘è‰²
        "ç¾¡æ…•": (50, 205, 50),   # æµ…ç»¿
        "å«‰å¦’": (34, 139, 34),   # æ£®æ—ç»¿
        "ç„¦è™‘": (75, 0, 130),    # é›è“
        "å…´å¥‹": (255, 165, 0),   # æ©™è‰²
        "å¹³é™": (173, 216, 230), # æµ…è“
        "æ€€æ—§": (186, 85, 211),  # ä¸­ç´«
        "åŒæƒ…": (0, 191, 255),   # æ·±å¤©è“
        "æ»¡è¶³": (60, 179, 113),  # ä¸­æµ·ç»¿
        "ä¸­æ€§": (255, 255, 255)  # ç™½è‰²
    }
    
    for (x, y, w, h), emotion in zip(
        face_cascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)),
        emotions
    ):
        color = emotion_colors.get(emotion, (255, 255, 255))
        
        # ç»˜åˆ¶å¸¦èƒŒæ™¯çš„æ ‡ç­¾
        label_size, _ = cv2.getTextSize(emotion, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
        cv2.rectangle(marked_img, 
                     (x, y - label_size[1] - 10), 
                     (x + label_size[0], y), 
                     color, cv2.FILLED)
        cv2.putText(marked_img, emotion, (x, y-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ç»˜åˆ¶äººè„¸æ¡†
        cv2.rectangle(marked_img, (x, y), (x+w, y+h), color, 2)
    
    return marked_img

def process_video(video_path):
    """å¤„ç†è§†é¢‘æ–‡ä»¶"""
    cap = cv2.VideoCapture(video_path)
    stframe = st.empty()
    stop_button = st.button("åœæ­¢å¤„ç†")
    
    while cap.isOpened() and not stop_button:
        ret, frame = cap.read()
        if not ret:
            break
            
        marked_frame = process_frame(frame)
        stframe.image(marked_frame, channels="BGR")
        
    cap.release()
    if stop_button:
        st.warning("è§†é¢‘å¤„ç†å·²ä¸­æ–­")

def process_uploaded_file(uploaded_file):
    """è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æˆ–è§†é¢‘"""
    file_type = uploaded_file.type.split('/')[0]
    
    if file_type == "image":
        image = Image.open(uploaded_file)
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        emotions = detect_emotion(img)
        
        emotion_count = {}
        for e in emotions:
            emotion_count[e] = emotion_count.get(e, 0) + 1
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("æƒ…ç»ªç»Ÿè®¡")
            if emotion_count:
                result_text = "ï¼Œ".join([f"{emotion}: {count}äºº" for emotion, count in emotion_count.items()])
                st.success(result_text)
            else:
                st.warning("æœªæ£€æµ‹åˆ°äººè„¸")
        
        with col2:
            tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
            with tab1:
                st.image(image, use_container_width=True)
            with tab2:
                marked_img = process_frame(img)
                st.image(marked_img, channels="BGR", use_container_width=True)
    
    elif file_type == "video":
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:
            tmpfile.write(uploaded_file.read())
            video_path = tmpfile.name
        
        st.info("è§†é¢‘å¤„ç†ä¸­...")
        process_video(video_path)

def main():
    st.set_page_config(page_title="æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ", layout="centered")
    st.title("ğŸ“Š æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘ï¼ˆJPG/PNG/MP4ï¼‰", 
        type=["jpg", "png", "jpeg", "mp4"],
        key="file_uploader"
    )
    
    if uploaded_file:
        process_uploaded_file(uploaded_file)

if __name__ == "__main__":
    main()
