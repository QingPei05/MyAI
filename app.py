import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile
import os
from moviepy.editor import VideoFileClip
import pandas as pd
import matplotlib.pyplot as plt

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

# ä¸­è‹±æ–‡æƒ…ç»ªæ ‡ç­¾
EMOTION_LABELS = {
    "happy": {"en": "happy", "cn": "é«˜å…´"},
    "excited": {"en": "excited", "cn": "å…´å¥‹"},
    "sad": {"en": "sad", "cn": "æ‚²ä¼¤"},
    "neutral": {"en": "neutral", "cn": "å¹³é™"}
}

# æƒ…ç»ªå¯¹åº”é¢œè‰²
EMOTION_COLORS = {
    "happy": (0, 255, 0),      # ç»¿è‰²
    "excited": (0, 255, 255),  # é»„è‰²
    "sad": (0, 0, 255),        # çº¢è‰²
    "neutral": (255, 255, 0)   # é’è‰²
}

def detect_emotion(frame):
    """åˆ†æå•å¸§å›¾åƒçš„æƒ…ç»ª"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    emotions = []
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=20)
        eyes = eye_cascade.detectMultiScale(roi_gray)
        
        # æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        if len(smiles) > 3:
            emotions.append("excited")
        elif len(smiles) > 0:
            emotions.append("happy")
        elif len(eyes) > 0 and eyes[0][1] / h < 0.3:
            emotions.append("sad")
        else:
            emotions.append("neutral")
    
    return emotions

def mark_emotion_on_image(img, emotions):
    """åœ¨å›¾ç‰‡ä¸Šæ ‡è®°æƒ…ç»ª"""
    marked_img = img.copy()
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    for (x, y, w, h), emotion in zip(faces, emotions):
        color = EMOTION_COLORS.get(emotion, (255, 255, 255))
        cv2.rectangle(marked_img, (x, y), (x+w, y+h), color, 2)
        # æ˜¾ç¤ºä¸­è‹±æ–‡æ ‡ç­¾
        label = f"{EMOTION_LABELS[emotion]['en']}/{EMOTION_LABELS[emotion]['cn']}"
        cv2.putText(marked_img, label, (x, y-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
    
    return marked_img

def process_image(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡"""
    image = Image.open(uploaded_file)
    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    emotions = detect_emotion(img)
    
    # ç»Ÿè®¡æƒ…ç»ª
    emotion_count = {e: emotions.count(e) for e in set(emotions)}
    
    # æ–°å¸ƒå±€ï¼šå·¦ä¾§ç»Ÿè®¡ï¼Œå³ä¾§å›¾ç‰‡
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("ğŸ“Š æƒ…ç»ªç»Ÿè®¡")
        if emotion_count:
            # åˆ›å»ºé¥¼å›¾
            fig, ax = plt.subplots()
            ax.pie(
                emotion_count.values(),
                labels=[EMOTION_LABELS[e]["cn"] for e in emotion_count.keys()],
                colors=[np.array(EMOTION_COLORS[e])/255 for e in emotion_count.keys()],
                autopct='%1.1f%%'
            )
            st.pyplot(fig)
            
            # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
            result_text = "ï¼Œ".join(
                [f"{count}äºº{EMOTION_LABELS[emotion]['cn']}" 
                 for emotion, count in emotion_count.items()]
            )
            st.success(f"**æ£€æµ‹ç»“æœ**: {result_text}")
        else:
            st.warning("æœªæ£€æµ‹åˆ°äººè„¸")
    
    with col2:
        # å¹¶æ’æ˜¾ç¤ºåŸå›¾å’Œåˆ†æç»“æœ
        tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
        with tab1:
            st.image(image, use_container_width=True)
        with tab2:
            if emotions:
                marked_img = mark_emotion_on_image(img, emotions)
                st.image(marked_img, channels="BGR", use_container_width=True)
            else:
                st.image(image, use_container_width=True)

def process_video(uploaded_file):
    """å¤„ç†ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶"""
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
        tmp_file.write(uploaded_file.read())
        video_path = tmp_file.name
    
    # è¯»å–è§†é¢‘
    clip = VideoFileClip(video_path)
    duration = clip.duration
    st.info(f"ğŸ¥ è§†é¢‘ä¿¡æ¯: é•¿åº¦ {duration:.2f}ç§’, {clip.fps:.2f} FPS")
    
    # è®¾ç½®é‡‡æ ·å¸§æ•°
    sample_freq = min(2, clip.fps)  # æ¯ç§’æœ€å¤šé‡‡æ ·2å¸§
    total_frames = int(duration * sample_freq)
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("å‡†å¤‡å¼€å§‹è§†é¢‘åˆ†æ...")
    
    # åˆ†æè§†é¢‘å¸§
    emotions_over_time = []
    sample_frames = []
    
    for i, frame in enumerate(clip.iter_frames(fps=sample_freq)):
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        emotions = detect_emotion(frame)
        emotions_over_time.extend(emotions)
        
        # æ¯5å¸§ä¿å­˜ä¸€ä¸ªæ ·æœ¬ç”¨äºå±•ç¤º
        if i % 5 == 0 and emotions:
            marked_frame = mark_emotion_on_image(frame, emotions)
            sample_frames.append(marked_frame)
        
        # æ›´æ–°è¿›åº¦
        progress = (i + 1) / total_frames
        progress_bar.progress(progress)
        status_text.text(f"åˆ†æè¿›åº¦: {int(progress*100)}% å®Œæˆ ({i+1}/{total_frames}å¸§)")
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    st.subheader("ğŸ“ˆ è§†é¢‘æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    
    if emotions_over_time:
        # æƒ…ç»ªé¢‘ç‡ç»Ÿè®¡
        emotion_count = {e: emotions_over_time.count(e) for e in set(emotions_over_time)}
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºæ˜¾ç¤º
        df = pd.DataFrame.from_dict(
            {EMOTION_LABELS[e]["cn"]: count for e, count in emotion_count.items()},
            orient='index',
            columns=['å‡ºç°æ¬¡æ•°']
        )
        
        # ä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("### æƒ…ç»ªå‡ºç°é¢‘ç‡")
            st.dataframe(df.style.background_gradient(cmap='Blues'))
            
        with col2:
            st.write("### æƒ…ç»ªåˆ†å¸ƒæ¯”ä¾‹")
            fig, ax = plt.subplots()
            ax.pie(
                df['å‡ºç°æ¬¡æ•°'],
                labels=df.index,
                colors=[np.array(EMOTION_COLORS[e])/255 for e in emotion_count.keys()],
                autopct='%1.1f%%'
            )
            st.pyplot(fig)
        
        # æ˜¾ç¤ºæ ·æœ¬å¸§
        st.write("### è§†é¢‘åˆ†ææ ·ä¾‹")
        cols = st.columns(min(3, len(sample_frames)))
        for idx, frame in enumerate(sample_frames[:3]):
            cols[idx].image(frame, channels="BGR", use_container_width=True)
        
        # æƒ…ç»ªå˜åŒ–è¶‹åŠ¿å›¾
        st.write("### æƒ…ç»ªå˜åŒ–è¶‹åŠ¿")
        timeline = pd.DataFrame({
            "æ—¶é—´ç‚¹": [i/sample_freq for i in range(len(emotions_over_time))],
            "æƒ…ç»ª": [EMOTION_LABELS[e]["cn"] for e in emotions_over_time]
        })
        st.line_chart(timeline.groupby(["æ—¶é—´ç‚¹", "æƒ…ç»ª"]).size().unstack().fillna(0))
    else:
        st.warning("âš ï¸ è§†é¢‘ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    clip.close()
    os.unlink(video_path)

def process_uploaded_file(uploaded_file):
    """è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æˆ–è§†é¢‘"""
    try:
        file_type = uploaded_file.type.split('/')[0]
        
        if file_type == "image":
            process_image(uploaded_file)
        elif file_type == "video":
            process_video(uploaded_file)
        else:
            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹")
    except Exception as e:
        st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

def main():
    st.set_page_config(
        page_title="æ™ºèƒ½æƒ…ç»ªåˆ†æç³»ç»Ÿ",
        layout="wide",
        page_icon="ğŸ˜Š"
    )
    
    st.title("ğŸ˜Š æ™ºèƒ½æƒ…ç»ªåˆ†æç³»ç»Ÿ")
    st.markdown("""
        <style>
        .stProgress > div > div > div > div {
            background-color: #1E90FF;
        }
        </style>
    """, unsafe_allow_html=True)
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘ï¼ˆJPG/PNG/MP4ï¼‰", 
        type=["jpg", "png", "jpeg", "mp4"],
        help="æ”¯æŒå•äººæˆ–å¤šäººçš„å›¾ç‰‡/è§†é¢‘åˆ†æ"
    )
    
    if uploaded_file:
        st.sidebar.info("æ–‡ä»¶ä¿¡æ¯", icon="â„¹ï¸")
        st.sidebar.write(f"æ–‡ä»¶å: {uploaded_file.name}")
        st.sidebar.write(f"æ–‡ä»¶ç±»å‹: {uploaded_file.type}")
        
        with st.spinner("åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
            process_uploaded_file(uploaded_file)

if __name__ == "__main__":
    main()
