import cv2
import numpy as np
import streamlit as st
from PIL import Image
import tempfile
from moviepy.editor import VideoFileClip

# åˆå§‹åŒ–æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

def detect_emotion(frame):
    """ä½¿ç”¨ç²¾ç¡®è§„åˆ™åˆ†æå•å¸§å›¾åƒçš„æƒ…ç»ª"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray = cv2.equalizeHist(gray)
    
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    
    emotions = []
    for (x, y, w, h) in faces:
        roi_gray = gray[y:y+h, x:x+w]
        
        # æ£€æµ‹é¢éƒ¨ç‰¹å¾
        smiles = smile_cascade.detectMultiScale(
            roi_gray, 
            scaleFactor=1.8, 
            minNeighbors=20,
            minSize=(25, 25))
        
        eyes = eye_cascade.detectMultiScale(
            roi_gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(20, 20))
        
        eye_count = len(eyes)
        smile_count = len(smiles)
        eye_sizes = [e[2] for e in eyes] if eye_count > 0 else [0]
        avg_eye_size = np.mean(eye_sizes) if eye_sizes else 0
        
        # ç²¾ç¡®çš„æƒ…ç»ªåˆ¤æ–­é€»è¾‘
        if smile_count > 3:
            emotions.append("å¿«ä¹")
        elif smile_count > 1:
            if eye_count >= 2 and avg_eye_size > h * 0.2:
                emotions.append("å…´å¥‹")
            else:
                emotions.append("æ»¡è¶³")
        elif smile_count > 0:
            emotions.append("å¹³é™")
        elif eye_count >= 2:
            if any(e[1] > h * 0.6 for e in eyes):  # çœ¼ç›ä½ç½®ä½
                emotions.append("æ‚²ä¼¤")
            elif any(e[1] < h * 0.3 for e in eyes):  # çœ¼ç›ä½ç½®é«˜
                if w > h * 0.85:  # å®½è„¸
                    emotions.append("æ„¤æ€’")
                else:
                    emotions.append("éª„å‚²")
            else:
                if avg_eye_size > h * 0.22:  # å¤§çœ¼ç›
                    emotions.append("æƒŠè®¶")
                else:
                    emotions.append("ä¸­æ€§")
        else:
            # æ ¹æ®è„¸éƒ¨ç‰¹å¾åˆ¤æ–­
            if w > h * 0.85:  # å®½è„¸
                emotions.append("æ„¤æ€’")
            elif h > w * 1.4:  # é•¿è„¸
                emotions.append("æ‚²ä¼¤")
            else:
                emotions.append("ä¸­æ€§")
    
    return emotions

def process_frame(frame):
    """å¤„ç†å•å¸§å›¾åƒå¹¶è¿”å›æ ‡è®°åçš„å›¾åƒ"""
    emotions = detect_emotion(frame)
    marked_img = frame.copy()
    
    # ç²¾ç®€åçš„æƒ…ç»ªé¢œè‰²æ˜ å°„
    emotion_colors = {
        "å¿«ä¹": (0, 255, 0),      # ç»¿è‰²
        "æ‚²ä¼¤": (0, 0, 255),      # çº¢è‰²
        "æ„¤æ€’": (0, 0, 139),     # æ·±çº¢
        "éª„å‚²": (255, 215, 0),   # é‡‘è‰²
        "å…´å¥‹": (255, 165, 0),   # æ©™è‰²
        "æ»¡è¶³": (60, 179, 113),  # ç»¿è‰²
        "å¹³é™": (173, 216, 230), # æµ…è“
        "æƒŠè®¶": (255, 255, 0),   # é’è‰²
        "ä¸­æ€§": (255, 255, 255)  # ç™½è‰²
    }
    
    for (x, y, w, h), emotion in zip(
        face_cascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)),
        emotions
    ):
        color = emotion_colors.get(emotion, (255, 255, 255))
        
        # ç»˜åˆ¶äººè„¸æ¡†
        cv2.rectangle(marked_img, (x, y), (x+w, y+h), color, 2)
        
        # åœ¨è„¸æ—æ·»åŠ æƒ…ç»ªæ ‡ç­¾ï¼ˆå¸¦èƒŒæ™¯ï¼‰
        label = f"{emotion}"
        font_scale = 0.9 if w > 60 else 0.7
        thickness = 2 if w > 60 else 1
        
        (label_width, label_height), baseline = cv2.getTextSize(
            label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
        
        # æ ‡ç­¾èƒŒæ™¯
        cv2.rectangle(marked_img,
                     (x, y - label_height - 10),
                     (x + label_width, y),
                     color, cv2.FILLED)
        
        # æ ‡ç­¾æ–‡å­—
        cv2.putText(marked_img, label,
                   (x, y - 10),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   font_scale, (0, 0, 0),  # é»‘è‰²æ–‡å­—
                   thickness, cv2.LINE_AA)
    
    return marked_img

def process_video(video_path):
    """å¤„ç†è§†é¢‘æ–‡ä»¶"""
    cap = cv2.VideoCapture(video_path)
    stframe = st.empty()
    stop_button = st.button("åœæ­¢å¤„ç†")
    
    while cap.isOpened() and not stop_button:
        ret, frame = cap.read()
        if not ret:
            break
            
        marked_frame = process_frame(frame)
        stframe.image(marked_img, channels="BGR")
        
    cap.release()
    if stop_button:
        st.warning("è§†é¢‘å¤„ç†å·²ä¸­æ–­")

def process_uploaded_file(uploaded_file):
    """è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æˆ–è§†é¢‘"""
    file_type = uploaded_file.type.split('/')[0]
    
    if file_type == "image":
        image = Image.open(uploaded_file)
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        emotions = detect_emotion(img)
        
        emotion_count = {}
        for e in emotions:
            emotion_count[e] = emotion_count.get(e, 0) + 1
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("æƒ…ç»ªç»Ÿè®¡")
            if emotion_count:
                # ä¸­æ–‡æƒ…ç»ªæ’åº
                emotion_order = ["å¿«ä¹", "å…´å¥‹", "æ»¡è¶³", "å¹³é™", "éª„å‚²", 
                                "æƒŠè®¶", "ä¸­æ€§", "æ‚²ä¼¤", "æ„¤æ€’"]
                sorted_emotions = sorted(emotion_count.items(),
                                       key=lambda x: emotion_order.index(x[0]) 
                result_text = "\n".join([f"â€¢ {emotion}: {count}äºº" for emotion, count in sorted_emotions])
                st.success(result_text)
            else:
                st.warning("æœªæ£€æµ‹åˆ°äººè„¸")
        
        with col2:
            tab1, tab2 = st.tabs(["åŸå§‹å›¾ç‰‡", "åˆ†æç»“æœ"])
            with tab1:
                st.image(image, use_column_width=True)
            with tab2:
                marked_img = process_frame(img)
                st.image(marked_img, channels="BGR", use_column_width=True)
    
    elif file_type == "video":
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmpfile:
            tmpfile.write(uploaded_file.read())
            video_path = tmpfile.name
        
        st.info("è§†é¢‘å¤„ç†ä¸­...")
        process_video(video_path)

def main():
    st.set_page_config(
        page_title="é«˜çº§æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ",
        page_icon="ğŸ˜Š",
        layout="wide"
    )
    
    st.title("ğŸ˜Š é«˜çº§æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    st.caption("ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘è¿›è¡Œå¤šæƒ…ç»ªæ£€æµ‹åˆ†æ")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©å›¾ç‰‡æˆ–è§†é¢‘æ–‡ä»¶ï¼ˆJPG/PNG/MP4ï¼‰",
        type=["jpg", "png", "jpeg", "mp4"],
        accept_multiple_files=False
    )
    
    if uploaded_file:
        with st.spinner("åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
            process_uploaded_file(uploaded_file)

if __name__ == "__main__":
    main()
